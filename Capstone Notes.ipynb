{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "tokenized_speech = tokenizer.tokenize(clinton_speech)\n",
    "print(tokenized_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(tokenized_speech)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "fdist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_speech=[]\n",
    "for w in tokenized_speech:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_speech.append(w.lower())\n",
    "print(\"Filtered Sentence:\",filtered_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenized_speech))\n",
    "print(len(filtered_speech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(filtered_speech)\n",
    "plt.figure(figsize=(10,10))\n",
    "fdist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_speech=[]\n",
    "for w in filtered_speech:\n",
    "    lemmatized_speech.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "len(set(lemmatized_speech))\n",
    "print(lemmatized_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(lemmatized_speech)\n",
    "plt.figure(figsize=(10,10))\n",
    "fdist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_ratings['Start Date'] = pd.to_numeric(trump_ratings['Start Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_ratings['End Date'] = pd.to_numeric(trump_ratings['End Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_ratings = trump_ratings.set_index('Start Date', append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_ratings.index.to_julian_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = new_df['Transcript']\n",
    "target = new_df['Party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = [d.split() for d in data.to_list()]\n",
    "print(processed_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = set()\n",
    "for comment in processed_data:\n",
    "    total_vocab.update([c.lower() for c in comment])\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output = []\n",
    "\n",
    "for listy in processed_data:\n",
    "    lemmed = ' '.join([lemmatizer.lemmatize(w) for w in listy])\n",
    "    lemmatized_output.append(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = lemmatized_output\n",
    "\n",
    "y_lem = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "sw_list = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘', '©',\n",
    "            'said', 'one', 'com', 'satirewire', '-', '–', '—', 'satirewire.com']\n",
    "sw_set = set(sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sw_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_dem = new_df[new_df['Party']== 'Democratic']\n",
    "df_freq_rep = new_df[new_df['Party']== 'Republican']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dem = df_freq_dem['Transcript']\n",
    "data_rep = df_freq_rep['Transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros_dem = [d.split() for d in data_dem.to_list()]\n",
    "pros_rep = [d.split() for d in data_rep.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dem = set()\n",
    "for comment in pros_dem:\n",
    "    total_dem.update([c.lower() for c in comment])\n",
    "len(total_dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rep = set()\n",
    "for comment in pros_rep:\n",
    "    total_rep.update([c.lower() for c in comment])\n",
    "len(total_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_party = [item.lower() for sublist in pros_dem for item in sublist if item not in sw_list ]\n",
    "rep_party = [item.lower() for sublist in pros_rep for item in sublist if item not in sw_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_freq = FreqDist(dem_party)\n",
    "rep_freq = FreqDist(rep_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_total_word_count = sum(dem_freq.values())\n",
    "dem_top_25 = dem_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in dem_top_25:\n",
    "    normalized_frequency = word[1]/dem_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_total_word_count = sum(rep_freq.values())\n",
    "rep_top_25 = rep_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in rep_top_25:\n",
    "    normalized_frequency = word[1]/rep_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_bar_counts = [x[1] for x in dem_freq.most_common(25)]\n",
    "dem_bar_words = [x[0] for x in dem_freq.most_common(25)]\n",
    "\n",
    "rep_bar_counts = [x[1] for x in rep_freq.most_common(25)]\n",
    "rep_bar_words = [x[0] for x in rep_freq.most_common(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "color = cm.viridis_r(np.linspace(.4,.8, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_figure = plt.figure(figsize=(16,4))\n",
    "\n",
    "ax = new_figure.add_subplot(121)\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "# Generate a line plot on first axes\n",
    "ax.bar(dem_bar_words, dem_bar_counts, color=color)\n",
    "# ax.plot(colormap='PRGn')\n",
    "\n",
    "# Draw a scatter plot on 2nd axes\n",
    "ax2.bar(rep_bar_words, rep_bar_counts, color=color )\n",
    "\n",
    "ax.title.set_text('Democrat')\n",
    "ax2.title.set_text('Republican')\n",
    "\n",
    "for ax in new_figure.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=60)\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# plt.savefig('word count bar graphs.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.20, random_state=1)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=sw_set)\n",
    "\n",
    "tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "tfidf_data_test_lem = tfidf.transform(X_test_lem)\n",
    "\n",
    "tfidf_data_train_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_cols = tfidf_data_train_lem.nnz / float(tfidf_data_train_lem.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train_lem.shape[1]))\n",
    "print('Percentage of columns containing ZERO: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_lem = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_lem.fit(tfidf_data_train_lem, y_train_lem)\n",
    "\n",
    "rf_train_preds_lem = rf_classifier_lem.predict(tfidf_data_train_lem)\n",
    "rf_test_preds_lem = rf_classifier_lem.predict(tfidf_data_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train_lem, rf_train_preds_lem))\n",
    "print(classification_report(y_test_lem, rf_test_preds_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf_classifier_lem, tfidf_data_train_lem, y_train_lem);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf_classifier_lem, tfidf_data_test_lem, y_test_lem);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = sorted(list(zip(rf_classifier_lem.feature_importances_, tfidf.get_feature_names())))[-20:]\n",
    "impts = pd.DataFrame(importances, columns=['impt', 'feat'])\n",
    "plt.barh(impts.feat, impts.impt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.reset_index()\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = new_data.join(new_df, how='inner')\n",
    "joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘', '©',]\n",
    "sw_set = set(sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english')\n",
    "speech_cv = vec.fit_transform(inaugural_speeches)\n",
    "\n",
    "inaugural_speech = pd.DataFrame(speech_cv.toarray(), columns = vec.get_feature_names())\n",
    "inaugural_speech.index = new_df.index\n",
    "inaugural_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = join(clinton_speeches, clinton_ratings, on=President, kind=left)\n",
    "clinton_merge = clinton_merge[clinton_merge[:, :dateStart] <= clinton_merge[:, :date] <= clinton_merge[:, :dateEnd], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_speeches = clinton_speeches.assign(key=1)\n",
    "clinton_ratings = clinton_ratings.assign(key=1)\n",
    "clinton_merge = pd.merge(clinton_speeches, clinton_ratings, on='key').drop('key',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_merge = clinton_merge.query('Date' >= 'Start_Date' | 'Date' <= 'End_Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_clinton = pd.merge(clinton_speeches, clinton_ratings, how='inner', left_on='cusip', right_on='ncusip')\n",
    "combined_clinton = df[(df['fdate']>=df['namedt']) & (df['fdate']<=df['nameenddt'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_ratings.Start_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_ratings.End_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "tf = TfidfVectorizer(lowercase=True, stop_words='english', tokenizer=token.tokenize)\n",
    "text_tf = tf.fit_transform(clinton_speeches['Transcript'])\n",
    "\n",
    "clinton_word_freq = pd.DataFrame(text_tf.todense(),columns = tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_freq = FreqDist(clinton_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_total_word_count = sum(clinton_freq.values())\n",
    "clinton_top_25 = clinton_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in clinton_top_25:\n",
    "    normalized_frequency = word[1]/clinton_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_bar_counts = [x[1] for x in clinton_freq.most_common(25)]\n",
    "clinton_bar_words = [x[0] for x in clinton_freq.most_common(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = cm.viridis_r(np.linspace(.4,.8, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a scatter plot on 2nd axes\n",
    "ax2.bar(not_satire_bar_words, not_satire_bar_counts, color=color )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.title.set_text('Not Satire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = new_figure.add_subplot(122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_ratings_End_Month = pd.to_datetime(clinton_ratings['End_Date'].dt.strftime('%Y-%m'))\n",
    "clinton_ratings_End_Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_merged_df = clinton_ratings.merge(clinton_speeches, how='inner', on='month_year')\n",
    "clinton_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_ratings['President'] = 'Bill Clinton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_speeches = clinton_speeches.assign(key=1)\n",
    "clinton_ratings= clinton_ratings.assign(key=1)\n",
    "df_merge = pd.merge(clinton_speeches, clinton_ratings, on='key').drop('key',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_merge.query('Date >= Start_Date and Date <= End_Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = clinton_ratings.merge(df_merge, on=['Start_Date','End_Date'], how='left').fillna('').drop('key', axis=1)\n",
    "\n",
    "print(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\":memory:\") \n",
    "clinton_speeches.to_sql('clinton_speeches', conn, index=False)\n",
    "clinton_ratings.to_sql('clinton_ratings', conn, index=False)\n",
    "qry = 'SELECT * FROM clinton_ratings, clinton_speeches WHERE clinton_speeches.Date <= clinton_ratings.Start_Date and clinton_speeches.Date <= clinton_ratings.End_Date'\n",
    "clinton_merged_df = pd.read_sql_query(qry,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(':memory:') \n",
    "clinton_speeches.to_sql('clinton_speeches', conn, index=False)\n",
    "clinton_ratings.to_sql('clinton_ratings', conn, index=False)\n",
    "qry = 'SELECT * FROM clinton_speeches, clinton_ratings WHERE clinton_speeches.Date BETWEEN clinton_ratings.Start_Date and clinton_ratings.End_Date'\n",
    "clinton_df = pd.read_sql_query(qry,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_clinton_df = pd.merge(clinton_speeches, clinton_ratings, how='inner', left_on='President', right_on='President')\n",
    "combined_clinton_df = combined_clinton_df[(combined_clinton_df['Date']>=combined_clinton_df['Start Date']) & (combined_clinton_df['Date']<=combined_clinton_df['End Date'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_speeches['dummy'] = 1\n",
    "clinton_ratings['dummy'] = 1\n",
    "merged_df = pd.merge(clinton_speeches,clinton_ratings,on='dummy')\n",
    "Result = merged_df[merged_df.Date.between(merged_df.Start_Date,merged_df.End_Date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_speech = new_df.Transcript[0]\n",
    "clinton_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_speech(clinton_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bush_ratings['President'] = 'George W. Bush'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(lemmatized_speech)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    fdist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized_speech=[]\n",
    "    for w in tokenized_speech:\n",
    "        if w.lower() not in sw_set:\n",
    "            lemmatized_speech.append(lemmatizer.lemmatize(w.lower()))\n",
    "\n",
    "    # len(set(lemmatized_speech))\n",
    "    # print(lemmatized_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(':memory:') \n",
    "bush_speeches.to_sql('bush_speeches', conn, index=False)\n",
    "bush_ratings.to_sql('bush_ratings', conn, index=False)\n",
    "qry = 'SELECT * FROM bush_speeches, bush_ratings WHERE bush_speeches.Date BETWEEN bush_ratings.Start_Date and bush_ratings.End_Date'\n",
    "bush_merged_df = pd.read_sql_query(qry,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bush_speech = new_df.Transcript[39]\n",
    "bush_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_speech(bush_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_speech = new_df.Transcript[80]\n",
    "obama_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_speech(obama_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_speech = new_df.Transcript[128]\n",
    "trump_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_speech(trump_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = obama_ratings.drop('high', axis=1)\n",
    "y = obama_ratings[['high']]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidential_ratings = [clinton_ratings, bush_ratings, obama_ratings, trump_ratings]\n",
    "presidential_ratings\n",
    "new_data = pd.DataFrame()\n",
    "for i in presidential_ratings:\n",
    "    combined_ratings = combined_ratings.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_speech(Transcript):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "    tokenized_speech = tokenizer.tokenize(Transcript)\n",
    "    # print(tokenized_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = [d.split() for d in data.to_list()]\n",
    "print(processed_data[:2])\n",
    "\n",
    "total_vocab = set()\n",
    "for comment in processed_data:\n",
    "    total_vocab.update([c.lower() for c in comment])\n",
    "len(total_vocab)\n",
    "\n",
    "\n",
    "lemmatized_output = []\n",
    "\n",
    "for listy in processed_data:\n",
    "    lemmed = ' '.join([lemmatizer.lemmatize(w) for w in listy])\n",
    "    lemmatized_output.append(lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_satire = df[df['target']==1]\n",
    "df_freq_not_satire = df[df['target']==0]\n",
    "\n",
    "data_sat = df_freq_satire['body']\n",
    "data_not_sat = df_freq_not_satire['body']\n",
    "\n",
    "pros_satire = [d.split() for d in data_sat.to_list()]\n",
    "pros_not_satire = [d.split() for d in data_not_sat.to_list()]\n",
    "\n",
    "total_vocab_sat = set()\n",
    "for comment in pros_satire:\n",
    "    total_vocab_sat.update([c.lower() for c in comment])\n",
    "len(total_vocab_sat)\n",
    "\n",
    "total_vocab_NOT_sat = set()\n",
    "for comment in pros_not_satire:\n",
    "    total_vocab_NOT_sat.update([c.lower() for c in comment])\n",
    "len(total_vocab_NOT_sat)\n",
    "\n",
    "flat_satire = [item.lower() for sublist in pros_satire for item in sublist if item not in sw_list ]\n",
    "flat_not_satire = [item.lower() for sublist in pros_not_satire for item in sublist if item not in sw_list]\n",
    "\n",
    "satire_freq = FreqDist(flat_satire)\n",
    "not_satire_freq = FreqDist(flat_not_satire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satire_total_word_count = sum(satire_freq.values())\n",
    "satire_top_25 = satire_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in satire_top_25:\n",
    "    normalized_frequency = word[1]/satire_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_satire_total_word_count = sum(not_satire_freq.values())\n",
    "not_satire_top_25 = not_satire_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in not_satire_top_25:\n",
    "    normalized_frequency = word[1]/not_satire_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satire_bar_counts = [x[1] for x in satire_freq.most_common(25)]\n",
    "satire_bar_words = [x[0] for x in satire_freq.most_common(25)]\n",
    "\n",
    "not_satire_bar_counts = [x[1] for x in not_satire_freq.most_common(25)]\n",
    "not_satire_bar_words = [x[0] for x in not_satire_freq.most_common(25)]\n",
    "\n",
    "color = cm.viridis_r(np.linspace(.4,.8, 30))\n",
    "\n",
    "new_figure = plt.figure(figsize=(16,4))\n",
    "\n",
    "ax = new_figure.add_subplot(121)\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "# Generate a line plot on first axes\n",
    "ax.bar(satire_bar_words, satire_bar_counts, color=color)\n",
    "# ax.plot(colormap='PRGn')\n",
    "\n",
    "# Draw a scatter plot on 2nd axes\n",
    "ax2.bar(not_satire_bar_words, not_satire_bar_counts, color=color )\n",
    "\n",
    "ax.title.set_text('Satire')\n",
    "ax2.title.set_text('Not Satire')\n",
    "\n",
    "for ax in new_figure.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=60)\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# plt.savefig('word count bar graphs.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "satire_dictionary = dict(zip(satire_bar_words, satire_bar_counts))\n",
    "not_satire_dictionary = dict(zip(not_satire_bar_words, not_satire_bar_counts))\n",
    "\n",
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(satire_dictionary)\n",
    "\n",
    "# Display the generated image w/ matplotlib:\n",
    "\n",
    "plt.figure(figsize=(10,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# Uncomment the next line if you want to save your image:\n",
    "# plt.savefig('satire_wordcloud.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(not_satire_dictionary)\n",
    "\n",
    "plt.figure(figsize=(10,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "# plt.savefig('not_satire_wordcloud.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = lemmatized_output\n",
    "\n",
    "y_lem = target\n",
    "\n",
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.20, random_state=1)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=sw_set)\n",
    "\n",
    "tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "tfidf_data_test_lem = tfidf.transform(X_test_lem)\n",
    "\n",
    "tfidf_data_train_lem\n",
    "\n",
    "non_zero_cols = tfidf_data_train_lem.nnz / float(tfidf_data_train_lem.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train_lem.shape[1]))\n",
    "print('Percentage of columns containing ZERO: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_lem = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "rf_classifier_lem.fit(tfidf_data_train_lem, y_train_lem)\n",
    "\n",
    "rf_train_preds_lem = rf_classifier_lem.predict(tfidf_data_train_lem)\n",
    "rf_test_preds_lem = rf_classifier_lem.predict(tfidf_data_test_lem)\n",
    "\n",
    "print(classification_report(y_train_lem, rf_train_preds_lem))\n",
    "print(classification_report(y_test_lem, rf_test_preds_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf_classifier_lem, tfidf_data_train_lem, y_train_lem);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf_classifier_lem, tfidf_data_test_lem, y_test_lem);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = sorted(list(zip(rf_classifier_lem.feature_importances_, tfidf.get_feature_names())))[-20:]\n",
    "impts = pd.DataFrame(importances, columns=['impt', 'feat'])\n",
    "plt.barh(impts.feat, impts.impt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pres_model(model, X_lem, y_lem):\n",
    "    X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.20, random_state=1)\n",
    "\n",
    "    tfidf = TfidfVectorizer(stop_words=sw_set)\n",
    "\n",
    "    tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "    tfidf_data_test_lem = tfidf.transform(X_test_lem)\n",
    "\n",
    "    tfidf_data_train_lem\n",
    "\n",
    "    non_zero_cols = tfidf_data_train_lem.nnz / float(tfidf_data_train_lem.shape[0])\n",
    "    \n",
    "    percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train_lem.shape[1]))\n",
    "\n",
    "    model.fit(tfidf_data_train_lem, y_train_lem)\n",
    "\n",
    "    train_pres_lem = model.predict(tfidf_data_train_lem)\n",
    "    test_pres_lem = model.predict(tfidf_data_test_lem)\n",
    "\n",
    "    print(classification_report(y_train_lem, train_pres_lem))\n",
    "    print(classification_report(y_test_lem, test_pres_lem))\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    plot_confusion_matrix(model, tfidf_data_train_lem, y_train_lem);\n",
    "    plot_confusion_matrix(model, tfidf_data_test_lem, y_test_lem);\n",
    "    \n",
    "    ax0.title.set_text('Train Confusion Matrix')\n",
    "    ax1.title.set_text('Test Confusion Matrix')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinton data codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_data = merged_clinton_df['Transcript']\n",
    "clinton_target = merged_clinton_df['high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = [d.split() for d in clinton_data.to_list()]\n",
    "print(processed_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_speeches = merged_clinton_df['Transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = set()\n",
    "for comment in processed_data:\n",
    "    total_vocab.update([c.lower() for c in comment])\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output = []\n",
    "\n",
    "for listy in processed_data:\n",
    "    lemmed = ' '.join([lemmatizer.lemmatize(w) for w in listy])\n",
    "    lemmatized_output.append(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_high = merged_clinton_df[merged_clinton_df['high']==1]\n",
    "df_freq_low = merged_clinton_df[merged_clinton_df['high']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_high = df_freq_high['Transcript']\n",
    "data_low = df_freq_low['Transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros_high = [d.split() for d in data_high.to_list()]\n",
    "pros_low = [d.split() for d in data_low.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_high = set()\n",
    "for comment in pros_high:\n",
    "    total_vocab_high.update([c.lower() for c in comment])\n",
    "len(total_vocab_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_low = set()\n",
    "for comment in pros_low:\n",
    "    total_vocab_low.update([c.lower() for c in comment])\n",
    "len(total_vocab_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline & Gridsearch setup\n",
    "# TFIDF pipeline setup\n",
    "tvc_pipe = Pipeline([\n",
    " ('tvec', TfidfVectorizer()),\n",
    " ('mb', MultinomialNB())\n",
    "])\n",
    "# Randomforest pipeline setup\n",
    "rf_pipe = Pipeline([\n",
    " ('tvec', TfidfVectorizer()),\n",
    " ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "log_pipe = Pipeline([\n",
    " ('tvec', TfidfVectorizer()),\n",
    " ('log', LogisticRegression())\n",
    "])\n",
    "dt_pipe = Pipeline([\n",
    " ('tvec', TfidfVectorizer()),\n",
    " ('log', DecisionTreeClassifier())\n",
    "])\n",
    "xgb_pipe = Pipeline([\n",
    " ('tvec', TfidfVectorizer()),\n",
    " ('xgb', XGBClassifier())\n",
    "])\n",
    "# Fit\n",
    "tvc_pipe.fit(X_train_lem, y_train_lem)\n",
    "rf_pipe.fit(X_train_lem, y_train_lem)\n",
    "# Setting params for TFIDF Vectorizer gridsearch\n",
    "tf_params = {\n",
    " 'tvec__max_features':[100, 2000],\n",
    " 'tvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    " 'tvec__stop_words': [None, 'english'],\n",
    " \n",
    "}\n",
    "# Setting up randomforest params\n",
    "rf_params = {\n",
    " 'tvec__max_features':[100],\n",
    " 'tvec__ngram_range': [(1, 2)],\n",
    " 'tvec__stop_words': ['english'],\n",
    " 'rf__max_depth': [5],\n",
    " 'rf__min_samples_split': [100],\n",
    " 'rf__max_leaf_nodes': [None]\n",
    "}\n",
    "dt_params = {\n",
    " 'tvec__max_features':[100],\n",
    " 'tvec__ngram_range': [(1, 2)],\n",
    " 'tvec__stop_words': ['english'],\n",
    " 'rf__max_depth': [5],\n",
    " 'rf__min_samples_split': [100],\n",
    " 'rf__max_leaf_nodes': [None]\n",
    "}\n",
    "xgb_params = {\n",
    " 'tvec__max_features':[100],\n",
    " 'tvec__ngram_range': [(1, 2)],\n",
    " 'tvec__stop_words': ['english'],\n",
    " 'rf__max_depth': [5],\n",
    " 'rf__min_samples_split': [100],\n",
    " 'rf__max_leaf_nodes': [None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up GridSearch for Randomforest\n",
    "rf_gs = GridSearchCV(rf_pipe, param_grid=rf_params, cv = 5, verbose = 1, n_jobs = -1)\n",
    "# Setting up GridSearch for TFIDFVectorizer\n",
    "tvc_gs = GridSearchCV(tvc_pipe, param_grid=tf_params, cv = 5, verbose =1, n_jobs = -1)\n",
    "dt_gs = GridSearchCV(dt_pipe, param_grid=dt_params, cv = 5, verbose =1, n_jobs = -1)\n",
    "xgb_gs = GridSearchCV(xgb_pipe, param_grid=xgb_params, cv = 5, verbose =1, n_jobs = -1)\n",
    "# Fitting TVC GS\n",
    "tvc_gs.fit(X_train_lem, y_train_lem)\n",
    "# Fitting Randomforest CV GS\n",
    "rf_gs.fit(X_train_lem, y_train_lem)\n",
    "dt_gs.fit(X_train_lem, y_train_lem)\n",
    "xgb_gs.fit(X_train_lem, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring Training data on TFIDFVectorizer\n",
    "tvc_gs.score(X_train, y_train)\n",
    "#score: 0.8742193813827052\n",
    "# Scoring Test data on TFIDFVectorizer\n",
    "tvc_gs.score(X_test, y_test)\n",
    "#score: 0.8627148523578669\n",
    "# Scoring Training data on RandomForest\n",
    "rf_gs.score(X_train, y_train)\n",
    "#score: 0.9380648005289839\n",
    "# Checking Test score on RandomForest\n",
    "rf_gs.score(X_test, y_test)\n",
    "#score: 0.881004847950639"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps = [(\"tfidf vectorization\", TfidfVectorizer()), (\"classifier\", MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{\"classifier\": [RandomForestClassifier]},\n",
    "                {\"classifier\": [MultinomialNB()]},\n",
    "                {\"classifier\": [LogisticRegression()],\n",
    "                \"classifier__solver\": [\"liblinear\"]},\n",
    "                {\"classifier\": [DecisionTreeClassifier()]},\n",
    "                {\"classifier\": [XGBClassifier()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'AUC': 'roc_auc', 'Accuracy': metrics.make_scorer(metrics.accuracy_score)}\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=search_space, cv=10,\n",
    "    scoring=scoring, return_train_score=True, n_jobs=-1, refit=\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.fit(X_train_lem, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (best_model.best_score_, best_model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_model.predict(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Model Score: %0.3f\" % metrics.accuracy_score(y_test_lem, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cm = metrics.confusion_matrix(y_test_lem, pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(best_cm, classes=['high', 'low'], title='Best Model Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    #\"vect__max_df\": (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    \"xgc__max_iter\": (20,),\n",
    "    \"xgc__alpha\": (0.00001, 0.000001),\n",
    "    \"xgc__penalty\": (\"l2\", \"elasticnet\"),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_freq_high = merged_obama_df[merged_obama_df['high']==1]\n",
    "obama_freq_low = merged_obama_df[merged_obama_df['high']==0]\n",
    "\n",
    "obama_high = obama_freq_high['Transcript']\n",
    "obama_low = obama_freq_low['Transcript']\n",
    "\n",
    "obama_pros_high = [d.split() for d in obam_high.to_list()]\n",
    "obama_pros_low = [d.split() for d in obama_low.to_list()]\n",
    "\n",
    "obama_flat_high = [item.lower() for sublist in obama_pros_high for item in sublist if item not in sw_list]\n",
    "obama_flat_low = [item.lower() for sublist in obama_pros_low for item in sublist if item not in sw_list]\n",
    "\n",
    "obama_high_freq = FreqDist(obama_flat_high)\n",
    "obama_low_freq = FreqDist(obama_flat_low)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_total_vocab_high = set()\n",
    "for comment in obama_pros_high:\n",
    "    obama_total_vocab_high.update([c.lower() for c in comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_total_vocab_low = set()\n",
    "for comment in bush_pros_low:\n",
    "    bush_total_vocab_low.update([c.lower() for c in comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_high_total_word_count = sum(obama_high_freq.values())\n",
    "obama_high_top_25 = obama_high_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in obama_high_top_25:\n",
    "    normalized_frequency = word[1]/obama_high_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_low_total_word_count = sum(obama_low_freq.values())\n",
    "obama_low_top_25 = obama_low_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in obama_low_top_25:\n",
    "    normalized_frequency = word[1]/obama_low_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_high_bar_counts = [x[1] for x in obama_high_freq.most_common(25)]\n",
    "obama_high_bar_words = [x[0] for x in obama_high_freq.most_common(25)]\n",
    "\n",
    "obama_low_bar_counts = [x[1] for x in obama_low_freq.most_common(25)]\n",
    "obama_low_bar_words = [x[0] for x in obama_low_freq.most_common(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = cm.viridis_r(np.linspace(.4,.8, 30))\n",
    "\n",
    "new_figure = plt.figure(figsize=(16,15))\n",
    "\n",
    "ax = new_figure.add_subplot(121)\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "# Generate a line plot on first axes\n",
    "ax.bar(obama_high_bar_words, obama_high_bar_counts, color=color)\n",
    "# ax.plot(colormap='PRGn')\n",
    "\n",
    "# Draw a scatter plot on 2nd axes\n",
    "ax2.bar(obama_low_bar_words, obama_low_bar_counts, color=color )\n",
    "\n",
    "ax.title.set_text('High Presidential Job Approval Ratings')\n",
    "ax2.title.set_text('Low Presidential Job Approval Ratings')\n",
    "\n",
    "for ax in new_figure.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=60)\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# plt.savefig('word count bar graphs.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_high_dictionary = dict(zip(obama_high_bar_words, obama_high_bar_counts))\n",
    "obama_low_dictionary = dict(zip(obama_low_bar_words, obama_low_bar_counts))\n",
    "\n",
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(obama_high_dictionary)\n",
    "\n",
    "# Display the generated image w/ matplotlib:\n",
    "\n",
    "plt.figure(figsize=(10,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# Uncomment the next line if you want to save your image:\n",
    "# plt.savefig('satire_wordcloud.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(obama_low_dictionary)\n",
    "\n",
    "# Display the generated image w/ matplotlib:\n",
    "\n",
    "plt.figure(figsize=(10,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# Uncomment the next line if you want to save your image:\n",
    "# plt.savefig('satire_wordcloud.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_speeches(Transcript):\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "    tokenized_speech = tokenizer.tokenize(Transcript)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    filtered_speech=[]\n",
    "    for w in tokenized_speech:\n",
    "        if w.lower() not in sw_set:\n",
    "            filtered_speech.append(w.lower())\n",
    "    \n",
    "    \n",
    "    lemmatized_speech=[]\n",
    "    for w in filtered_speech:\n",
    "        lemmatized_speech.append(lemmatizer.lemmatize(w))\n",
    "        \n",
    "    fdist = FreqDist(lemmatized_speech)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    print(fdist.plot(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_freq_words(preprocessed_speech):\n",
    "    \n",
    "    pres_freq_high = df[df['target']==1]\n",
    "    pres_freq_low = df[df['target']==0]\n",
    "\n",
    "    data_sat = df_freq_satire['body']\n",
    "    data_not_sat = df_freq_not_satire['body']\n",
    "\n",
    "\n",
    "    total_vocab_sat = set()\n",
    "    for comment in pros_satire:\n",
    "        total_vocab_sat.update([c.lower() for c in comment])\n",
    "\n",
    "    total_vocab_NOT_sat = set()\n",
    "    for comment in pros_not_satire:\n",
    "        total_vocab_NOT_sat.update([c.lower() for c in comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed_speech(Transcript):\n",
    "    \n",
    "    processed_data = [d.split() for d in data.to_list()]\n",
    "    \n",
    "\n",
    "    total_vocab = set()\n",
    "    for comment in processed_data:\n",
    "        total_vocab.update([c.lower() for c in comment])\n",
    "\n",
    "\n",
    "\n",
    "    lemmatized_output = []\n",
    "\n",
    "    for listy in processed_data:\n",
    "        lemmed = ' '.join([lemmatizer.lemmatize(w) for w in listy])\n",
    "        lemmatized_output.append(lemmed)\n",
    "        \n",
    "    fdist = FreqDist(lemmatized_speech)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    fdist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_clinton_df['high'] = merged_clinton_df['high'].astype(int)\n",
    "merged_clinton_df['low'] = merged_clinton_df['low'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_docs(speeches_ratings):\n",
    "    clinton_speeches['month_year'] = pd.to_datetime(clinton_speeches['Date']).dt.to_period('M')\n",
    "    merged_clinton_df = test_clinton.merge(clinton_speeches, how='inner', on='month_year')\n",
    "    return clinton_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_ratings['high'], clinton_ratings['low'] = (clinton_ratings.Approving >= 50).astype(int), (~(clinton_ratings.Approving >=50)).astype(int)\n",
    "clinton_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_ratings.rename(columns={'Start Date': 'Start_Date', 'End Date': 'End_Date'}, inplace=True)\n",
    "clinton_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_ratings['Start_Date'] = pd.to_datetime(clinton_ratings['Start_Date'])\n",
    "clinton_ratings['End_Date'] = pd.to_datetime(clinton_ratings['End_Date'])\n",
    "clinton_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_ratings['month_year'] = pd.to_datetime(clinton_ratings['End_Date']).dt.to_period('M')\n",
    "clinton_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_new_ratings = clinton_ratings.groupby('month_year').mean()\n",
    "clinton_new_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_new_ratings.reset_index(inplace=True)\n",
    "clinton_new_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_speeches['month_year'] = pd.to_datetime(clinton_speeches['Date']).dt.to_period('M')\n",
    "clinton_speeches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_clinton_df = clinton_new_ratings.merge(clinton_speeches, how='inner', on='month_year')\n",
    "merged_clinton_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bush_speeches['month_year'] = pd.to_datetime(bush_speeches['Date']).dt.to_period('M')\n",
    "bush_speeches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, X_lem, y_lem):\n",
    "    \n",
    "    X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.20, random_state=42)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(stop_words=sw_set)\n",
    "\n",
    "    tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "    tfidf_data_test_lem = tfidf.transform(X_test_lem)\n",
    "    \n",
    "    non_zero_cols = tfidf_data_train_lem.nnz / float(tfidf_data_train_lem.shape[0])\n",
    "    \n",
    "    percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train_lem.shape[1]))\n",
    "    \n",
    "    model.fit(tfidf_data_train_lem, y_train_lem)\n",
    "\n",
    "\n",
    "    y_hat_train = model.predict(tfidf_data_train_lem)\n",
    "    y_hat_test = model.predict(tfidf_data_test_lem)\n",
    "    \n",
    "    print(classification_report(y_train_lem, y_hat_train))\n",
    "    print(classification_report(y_test_lem, y_hat_test))\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    plot_confusion_matrix(model, tfidf_data_train_lem, y_train_lem, ax=ax0)\n",
    "    plot_confusion_matrix(model, tfidf_data_test_lem, y_test_lem, ax=ax1)\n",
    "\n",
    "    ax0.title.set_text('Train Confusion Matrix')\n",
    "    ax1.title.set_text('Test Confusion Matrix')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=100, random_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.extend(sorted(presidential_high, key=presidential_high.get, reverse=True)[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_ratings = []\n",
    "for word in words:\n",
    "    if word in presidential_high.keys():\n",
    "        high_ratings.append(presidential_high[word])\n",
    "    else:\n",
    "        high_ratings.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15, 15]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "labels = words\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - 1.5*width, csvalues, width, label='Clinton')\n",
    "rects2 = ax.bar(x - width/2, bsvalues, width, label='Bush')\n",
    "rects3 = ax.bar(x + width/2, osvalues, width, label='Obama')\n",
    "rects4 = ax.bar(x + 1.5*width, tsvalues, width, label='Trump')\n",
    "\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Presidential Word Counts')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "        xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "        xytext=(0, 3), # 3 points vertical offset\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "autolabel(rects4)\n",
    "plt.xticks(rotation=45)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), ha='right');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, X_lem, y_lem):\n",
    "    \n",
    "    X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.20, random_state=42)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(stop_words=sw_set)\n",
    "\n",
    "    tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "    tfidf_data_test_lem = tfidf.transform(X_test_lem)\n",
    "    \n",
    "    non_zero_cols = tfidf_data_train_lem.nnz / float(tfidf_data_train_lem.shape[0])\n",
    "    \n",
    "    percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train_lem.shape[1]))\n",
    "    \n",
    "    model.fit(tfidf_data_train_lem, y_train_lem)\n",
    "\n",
    "\n",
    "    y_hat_train = model.predict(tfidf_data_train_lem)\n",
    "    y_hat_test = model.predict(tfidf_data_test_lem)\n",
    "    \n",
    "    print(classification_report(y_train_lem, y_hat_train))\n",
    "    print(classification_report(y_test_lem, y_hat_test))\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    plot_confusion_matrix(model, tfidf_data_train_lem, y_train_lem, ax=ax0)\n",
    "    plot_confusion_matrix(model, tfidf_data_test_lem, y_test_lem, ax=ax1)\n",
    "\n",
    "    ax0.title.set_text('Train Confusion Matrix')\n",
    "    ax1.title.set_text('Test Confusion Matrix')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = sorted(list(zip(rf_classifier_lem.feature_importances_, tfidf.get_feature_names())))[-20:]\n",
    "impts = pd.DataFrame(importances, columns=['impt', 'feat'])\n",
    "plt.barh(impts.feat, impts.impt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_words(speech_words):\n",
    "    organized_words = dict(sorted(speech_words.items(), key=lambda item: item[1], reverse=True)[:15])\n",
    "    return speech_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15, 10]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "labels = words\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - 1.5*width, csvalues, width, label='Clinton')\n",
    "rects2 = ax.bar(x - width/2, bsvalues, width, label='Bush')\n",
    "rects3 = ax.bar(x + width/2, osvalues, width, label='Obama')\n",
    "rects4 = ax.bar(x + 1.5*width, tsvalues, width, label='Trump')\n",
    "\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Presidential Word Counts')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "        xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "        xytext=(0, 3), # 3 points vertical offset\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "autolabel(rects4)\n",
    "plt.xticks(rotation=45)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), ha='right');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsvalues.sort(reverse=True)\n",
    "osvalues.sort(reverse=True)\n",
    "tsvalues.sort(reverse=True)\n",
    "high_ratings.sort(reverse=True)\n",
    "low_ratings.sort(reverse=True)\n",
    "demo_values.sort(reverse=True)\n",
    "rep_values.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Association Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(cs, key=cs.get, reverse=True)[:15]\n",
    "words.extend(sorted(bs, key=bs.get, reverse=True)[:15])\n",
    "words.extend(sorted(os, key=os.get, reverse=True)[:15])\n",
    "words.extend(sorted(ts, key=ts.get, reverse=True)[:15])\n",
    "words.extend(sorted(pres_high, key=pres_high.get, reverse=True)[:15])\n",
    "words.extend(sorted(pres_low, key=pres_low.get, reverse=True)[:15])\n",
    "words.extend(sorted(demo, key=demo.get, reverse=True)[:15])\n",
    "words.extend(sorted(rep, key=rep.get, reverse=True)[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvalues = []\n",
    "bsvalues = []\n",
    "osvalues = []\n",
    "tsvalues = []\n",
    "high_ratings = []\n",
    "low_ratings = []\n",
    "demo_values = []\n",
    "rep_values = []\n",
    "\n",
    "\n",
    "for word in words:\n",
    "    # Clinton values\n",
    "    if word in cs_words.keys():\n",
    "        csvalues.append(cs_words[word])\n",
    "    else:\n",
    "        csvalues.append(0)\n",
    "    # Bush Values\n",
    "    if word in bs.keys():\n",
    "        bsvalues.append(bs[word])\n",
    "    else:\n",
    "        bsvalues.append(0)\n",
    "    # Obama values\n",
    "    if word in os.keys():\n",
    "        osvalues.append(os[word])\n",
    "    else:\n",
    "        osvalues.append(0)\n",
    "    # Trump Values\n",
    "    if word in ts.keys():\n",
    "        tsvalues.append(ts[word])\n",
    "    else:\n",
    "        tsvalues.append(0)\n",
    "        # High Rating Values\n",
    "    if word in pres_high.keys():\n",
    "        high_ratings.append(pres_high[word])\n",
    "    else:\n",
    "        high_ratings.append(0)\n",
    "        # Low Rating Values\n",
    "    if word in pres_low.keys():\n",
    "        low_ratings.append(pres_low[word])\n",
    "    else:\n",
    "        low_ratings.append(0)\n",
    "        # Democrat Values\n",
    "    if word in demo.keys():\n",
    "        demo_values.append(demo[word])\n",
    "    else:\n",
    "        demo_values.append(0)\n",
    "        # Republican Values\n",
    "    if word in rep.keys():\n",
    "        rep_values.append(rep[word])\n",
    "    else:\n",
    "        rep_values.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15, 10]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "labels = words\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x + width/2, csvalues, width)\n",
    "\n",
    "\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('President Clinton Word Counts')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "        xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "        xytext=(0, 3), # 3 points vertical offset\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), ha='right');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15, 15]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "labels = words\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects2 = ax.bar(x + width/2, bsvalues, width)\n",
    "\n",
    "\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('President Bush Word Counts')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "        xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "        xytext=(0, 3), # 3 points vertical offset\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), ha='right');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15, 10]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "labels = words\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects3 = ax.bar(x + width/2, osvalues, width)\n",
    "\n",
    "\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('President Obama Word Counts')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "        xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "        xytext=(0, 3), # 3 points vertical offset\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), ha='right');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15, 10]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "labels = words\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - 1.5*width, csvalues, width, label='Clinton')\n",
    "rects2 = ax.bar(x - width/2, bsvalues, width, label='Bush')\n",
    "rects3 = ax.bar(x + width/2, osvalues, width, label='Obama')\n",
    "rects4 = ax.bar(x + 1.5*width, tsvalues, width, label='Trump')\n",
    "\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Presidential Word Counts')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "        xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "        xytext=(0, 3), # 3 points vertical offset\n",
    "        textcoords=\"offset points\",\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "autolabel(rects4)\n",
    "plt.xticks(rotation=45)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), ha='right');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensionality of my data was sufficient enough for prediction and any word tokens that are added to the documents increased our prediction capabilities. Most of the words in the data are important, and the decrease in the accuracy rate proves it. This shows the previous model not testing for relevant terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.20, random_state=1)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=sw_set)\n",
    "\n",
    "tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "tfidf_data_test_lem = tfidf.transform(X_test_lem)\n",
    "\n",
    "tfidf_data_train_lem\n",
    "\n",
    "non_zero_cols = tfidf_data_train_lem.nnz / float(tfidf_data_train_lem.shape[0])\n",
    "\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train_lem.shape[1]))\n",
    "\n",
    "\n",
    "rf_classifier_lem.fit(tfidf_data_train_lem, y_train_lem)\n",
    "\n",
    "rf_train_preds_lem = rf_classifier_lem.predict(tfidf_data_train_lem)\n",
    "rf_test_preds_lem = rf_classifier_lem.predict(tfidf_data_test_lem)\n",
    "\n",
    "print(classification_report(y_train_lem, rf_train_preds_lem))\n",
    "print(classification_report(y_test_lem, rf_test_preds_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder   = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "bigram_finder.apply_freq_filter(3)\n",
    "bigram_finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![image](https://user-images.githubusercontent.com/77416319/152268337-4cf39516-2179-46c8-8686-df451f4b2889.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/77416319/152268429-3a63a262-3ed1-4a95-abee-f9565f1cbce0.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/77416319/152268477-f85bd096-e32c-4801-ab1e-acb354613c35.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/77416319/152268533-1d75d9d9-fbf8-451c-87d9-4e1043b3e25c.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/77416319/152268569-742adc80-abbd-4ea4-8351-dcbcf71b5835.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/77416319/152268645-18993f49-ba59-4b88-8e8f-7061046d8a00.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/77416319/152268709-38f35140-891e-495e-9edd-be3facfac8af.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/77416319/152268793-8c38caff-d036-42c2-9f93-84fe94008e08.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf = GridSearchCV(XGBoost_model, param_grid, scoring='accuracy', cv=None, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_clf2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf = GridSearchCV(XG_lem, param_grid)\n",
    "run_model(grid_clf, X_lem, y_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf = GridSearchCV(XG_lem, param_grid)\n",
    "grid_clf.fit(tfidf_data_train_lem, y_train_lem)\n",
    "\n",
    "best_parameters = grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "training_preds = grid_clf.predict(tfidf_data_train_lem)\n",
    "test_preds = grid_clf.predict(tfidf_data_test_lem)\n",
    "training_accuracy = accuracy_score(y_train_lem, training_preds)\n",
    "test_accuracy = accuracy_score(y_test_lem, test_preds)\n",
    "\n",
    "print('')\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = {\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [20],\n",
    "    'min_child_weight': [1, 2],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'n_estimators': [100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf2 = GridSearchCV(XG_lem, param_grid2, scoring='accuracy', cv=10, n_jobs=1)\n",
    "grid_clf2.fit(tfidf_data_train_lem, y_train_lem)\n",
    "\n",
    "best_parameters = grid_clf2.best_params_\n",
    "\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "training_preds = grid_clf2.predict(tfidf_data_train_lem)\n",
    "test_preds = grid_clf2.predict(tfidf_data_test_lem)\n",
    "training_accuracy = accuracy_score(y_train_lem, training_preds)\n",
    "test_accuracy = accuracy_score(y_test_lem, test_preds)\n",
    "\n",
    "print('')\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_transcript  = clinton_speeches[['Transcript']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_transcript['Lemmatized'] = clinton_transcript['Transcript'].apply(lambda x: preprocessed_speeches(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
